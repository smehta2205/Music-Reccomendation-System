{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prod2vec",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_vSQc_g3R1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIgld1MD4S1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_link_1=\"https://drive.google.com/open?id=1vx9pDj-Av55AlUe10Bn3DjxDqzHEdl-m\"\n",
        "initial1, train_id1 = train_link_1.split('=')\n",
        "\n",
        "downloaded1 = drive.CreateFile({'id':train_id1})\n",
        "downloaded1.GetContentFile('sessions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeu8Q_4q4yJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTwnE4qc4AXh",
        "colab_type": "text"
      },
      "source": [
        "**Loading and precprocessing the data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SML2zYlD7KEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import calendar\n",
        "import time\n",
        "import datetime\n",
        "def load_and_adapt(path, last_months=0):\n",
        "    data = pd.read_csv(path, header=0)\n",
        "    col_names = ['session_id', 'user_id', 'item_id', 'ts'] + data.columns.values.tolist()[4:]\n",
        "    data.columns = col_names\n",
        "\n",
        "    if last_months > 0:\n",
        "        def add_months(sourcedate, months):\n",
        "            month = sourcedate.month - 1 + months\n",
        "            year = int(sourcedate.year + month / 12)\n",
        "            month = month % 12 + 1\n",
        "            day = min(sourcedate.day, calendar.monthrange(year, month)[1])\n",
        "            return datetime.date(year, month, day)\n",
        "\n",
        "        lastdate = datetime.datetime.fromtimestamp(data.ts.max())\n",
        "        firstdate = add_months(lastdate, -last_months)\n",
        "        initial_unix = time.mktime(firstdate.timetuple())\n",
        "\n",
        "        # filter out older interactions\n",
        "        data = data[data['ts'] >= initial_unix]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUPUTP-5EMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "topk = 1000\n",
        "file = load_and_adapt(\"sessions.csv\", last_months=1)\n",
        "\n",
        "c = Counter(list(file['item_id']))\n",
        "\n",
        "if topk > 1:\n",
        "    keeper = set([x[0] for x in c.most_common(topk)])\n",
        "    file = file[file['item_id'].isin(keeper)]\n",
        "\n",
        "# group by session id and concat song_id\n",
        "groups = file.groupby('session_id')\n",
        "\n",
        "# convert item ids to string, then aggregate them to lists\n",
        "aggregated = groups['item_id'].aggregate([lambda x: list(map(str, x))])\n",
        "init_ts = groups['ts'].min()\n",
        "users = groups['user_id'].min()  \n",
        "\n",
        "result = aggregated.join(init_ts).join(users)\n",
        "result.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8E91_IBHXJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d4aa59ac-b94f-49c4-e351-51e3b0834f1c"
      },
      "source": [
        "result = result.rename(columns = {\"<lambda>\":\"sequence\"})\n",
        "result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>sequence</th>\n",
              "      <th>ts</th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>122</td>\n",
              "      <td>[1762, 3700, 638]</td>\n",
              "      <td>1420059172</td>\n",
              "      <td>2432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>223</td>\n",
              "      <td>[3772, 3953]</td>\n",
              "      <td>1419418147</td>\n",
              "      <td>15861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>226</td>\n",
              "      <td>[245, 1271, 379]</td>\n",
              "      <td>1419433841</td>\n",
              "      <td>15861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>243</td>\n",
              "      <td>[245, 1197, 4307, 3868]</td>\n",
              "      <td>1421674741</td>\n",
              "      <td>15861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>245</td>\n",
              "      <td>[409, 234, 2334, 2431, 231, 4738, 219, 2403]</td>\n",
              "      <td>1421679507</td>\n",
              "      <td>15861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65875</th>\n",
              "      <td>2764052</td>\n",
              "      <td>[419, 930, 419, 908, 3493, 5294, 5297, 5299, 5...</td>\n",
              "      <td>1421508739</td>\n",
              "      <td>4503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65876</th>\n",
              "      <td>2764159</td>\n",
              "      <td>[528, 6475]</td>\n",
              "      <td>1421059220</td>\n",
              "      <td>12934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65877</th>\n",
              "      <td>2764161</td>\n",
              "      <td>[6349, 2803]</td>\n",
              "      <td>1421141469</td>\n",
              "      <td>12934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65878</th>\n",
              "      <td>2764164</td>\n",
              "      <td>[1485, 5733, 1482, 2445, 915]</td>\n",
              "      <td>1421430665</td>\n",
              "      <td>12934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65879</th>\n",
              "      <td>2764165</td>\n",
              "      <td>[40, 3319, 1546, 1484, 1417, 155, 3495, 3188]</td>\n",
              "      <td>1421437865</td>\n",
              "      <td>12934</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65880 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       session_id  ... user_id\n",
              "0             122  ...    2432\n",
              "1             223  ...   15861\n",
              "2             226  ...   15861\n",
              "3             243  ...   15861\n",
              "4             245  ...   15861\n",
              "...           ...  ...     ...\n",
              "65875     2764052  ...    4503\n",
              "65876     2764159  ...   12934\n",
              "65877     2764161  ...   12934\n",
              "65878     2764164  ...   12934\n",
              "65879     2764165  ...   12934\n",
              "\n",
              "[65880 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01PIkh7I4HAH",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the data into Train and Test Datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYuoUU8HSh01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def last_session_out_split(data, user_key='user_id', session_key='session_id', time_key='ts'):\n",
        "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
        "    last_session = sessions.last()\n",
        "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
        "    test = data[data.session_id.isin(last_session.values)].copy()\n",
        "    train, test = clean_split(train, test)\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def clean_split(train, test):\n",
        "    train_items = set()\n",
        "    train['sequence'].apply(lambda seq: train_items.update(set(seq)))\n",
        "    test['sequence'] = test['sequence'].apply(lambda seq: [it for it in seq if it in train_items])\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMBJj96zTMwD",
        "colab_type": "code",
        "outputId": "35d5113d-4cd5-4b4c-f2ec-e00fdfce031f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data, test_data = last_session_out_split(result)\n",
        "print(\"Train sessions: {} - Test sessions: {}\".format(len(train_data), len(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train sessions: 48068 - Test sessions: 17812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_b_Uu24OCs",
        "colab_type": "text"
      },
      "source": [
        "**Training the model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GeWvbKBTSpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import gensim\n",
        "\n",
        "class Prod2VecRecommender():\n",
        "\n",
        "    def __init__(self, min_count=2, size=100, window=5, decay_alpha=0.9, workers=4):\n",
        "        #super(Prod2VecRecommender, self).__init__()\n",
        "        self.min_count = min_count\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.decay_alpha = decay_alpha\n",
        "        self.workers = workers\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'Prod2VecRecommender(min_count={min_count}, ' \\\n",
        "               'size={size}, ' \\\n",
        "               'window={window}, ' \\\n",
        "               'decay_alpha={decay_alpha}, ' \\\n",
        "               'workers={workers})'.format(**self.__dict__)\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        sequences = train_data['sequence'].values\n",
        "        self.model = gensim.models.Word2Vec(sequences, min_count=self.min_count, window=self.window, hs=1, size=self.size, sg=1, workers=self.workers)\n",
        "\n",
        "    def recommend(self, user_profile, user_id=None):\n",
        "        user_profile = list(map(str, user_profile))\n",
        "        rec = []\n",
        "        try:\n",
        "            # iterate the user profile backwards\n",
        "            for i, item in enumerate(user_profile[::-1]):\n",
        "                ms = self.model.most_similar(positive=item)\n",
        "                # apply exponential decay to the similarity scores\n",
        "                decay = self.decay_alpha ** i\n",
        "                ms = [(x[0], decay * x[1]) for x in ms]\n",
        "                rec.extend(ms)\n",
        "            # sort items by similarity score\n",
        "            rec = sorted(rec, key=lambda x: -x[1])\n",
        "        except KeyError:\n",
        "            rec = []\n",
        "        return [([x[0]], x[1]) for x in rec]\n",
        "\n",
        "    def get_recommendation_list(self, recommendation):\n",
        "        return list(map(lambda x: x[0], recommendation))\n",
        "\n",
        "    def get_recommendation_confidence_list(self, recommendation):\n",
        "        return list(map(lambda x: x[1], recommendation))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek3a0HVxVhmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recommender = Prod2VecRecommender(min_count=2, size=50, window=5, decay_alpha=0.9, workers=4)\n",
        "recommender.fit(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWTF2IEQ4YAw",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PokEsZ7lV_AA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GIVEN_K = 1\n",
        "LOOK_AHEAD = 1\n",
        "STEP = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWCUnu-1XEGZ",
        "colab_type": "code",
        "outputId": "bf9d4f5c-fed6-4e65-e888-d15f71a3fecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sequences = test_data.loc[test_data['sequence'].map(len) > abs(GIVEN_K), 'sequence'].values\n",
        "print('{} sequences available for evaluation'.format(len(test_sequences)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11776 sequences available for evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg80QEaOXabI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm    #showing the progress bar\n",
        "#evaluation method defined \n",
        "def sequential_evaluation(recommender, test_sequences, evaluation_functions, users=None, given_k=1, look_ahead=1, top_n=10, scroll=True, step=1):\n",
        "    if given_k == 0:\n",
        "        raise ValueError('given_k must be != 0')\n",
        "\n",
        "    metrics = np.zeros(len(evaluation_functions))\n",
        "    with tqdm(total=len(test_sequences)) as pbar:\n",
        "        for i, test_seq in enumerate(test_sequences):\n",
        "            if users is not None:\n",
        "                user = users[i]\n",
        "            else:\n",
        "                user = None\n",
        "            if scroll:\n",
        "                metrics += sequence_sequential_evaluation(recommender, test_seq, evaluation_functions, user, given_k, look_ahead, top_n, step)\n",
        "            else:\n",
        "                metrics += evaluate_sequence(recommender, test_seq, evaluation_functions, user, given_k, look_ahead, top_n)\n",
        "            pbar.update(1)\n",
        "    return metrics / len(test_sequences)\n",
        "\n",
        "\n",
        "def evaluate_sequence(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n):\n",
        "    # safety checks\n",
        "    if given_k < 0:\n",
        "        given_k = len(seq) + given_k\n",
        "\n",
        "    user_profile = seq[:given_k]\n",
        "    ground_truth = seq[given_k:]\n",
        "\n",
        "    # restrict ground truth to look_ahead\n",
        "    ground_truth = ground_truth[:look_ahead] if look_ahead != 'all' else ground_truth\n",
        "    ground_truth = list(map(lambda x: [x], ground_truth))  # list of list format\n",
        "\n",
        "    if not user_profile or not ground_truth:\n",
        "        # if any of the two missing all evaluation functions are 0\n",
        "        return np.zeros(len(evaluation_functions))\n",
        "\n",
        "    r = recommender.recommend(user_profile, user)[:top_n]\n",
        "\n",
        "    if not r:\n",
        "        # no recommendation found\n",
        "        return np.zeros(len(evaluation_functions))\n",
        "    reco_list = recommender.get_recommendation_list(r)\n",
        "\n",
        "    tmp_results = []\n",
        "    for f in evaluation_functions:\n",
        "        tmp_results.append(f(ground_truth, reco_list))\n",
        "    return np.array(tmp_results)\n",
        "  \n",
        "def sequence_sequential_evaluation(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n, step):\n",
        "    if given_k < 0:\n",
        "        given_k = len(seq) + given_k\n",
        "\n",
        "    eval_res = 0.0\n",
        "    eval_cnt = 0\n",
        "    for gk in range(given_k, len(seq), step):\n",
        "        eval_res += evaluate_sequence(recommender, seq, evaluation_functions, user, gk, look_ahead, top_n)\n",
        "        eval_cnt += 1\n",
        "    return eval_res / eval_cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vodYXGJieNg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metrics defined\n",
        "def precision(ground_truth, prediction):\n",
        "    ground_truth = remove_duplicates(ground_truth)\n",
        "    prediction = remove_duplicates(prediction)\n",
        "    precision_score = count_a_in_b_unique(prediction, ground_truth) / float(len(prediction))\n",
        "    assert 0 <= precision_score <= 1\n",
        "    return precision_score\n",
        "\n",
        "\n",
        "def recall(ground_truth, prediction):\n",
        "    ground_truth = remove_duplicates(ground_truth)\n",
        "    prediction = remove_duplicates(prediction)\n",
        "    if len(prediction) == 0:\n",
        "      recall_score = 0  \n",
        "    else:\n",
        "      recall_score = count_a_in_b_unique(prediction, ground_truth) / float(len(ground_truth))\n",
        "    assert 0 <= recall_score <= 1\n",
        "    return recall_score\n",
        "\n",
        "\n",
        "def mrr(ground_truth, prediction):\n",
        "    rr = 0\n",
        "    for rank, p in enumerate(prediction):\n",
        "        if p in ground_truth:\n",
        "            rr = 1 / (rank + 1)\n",
        "            break\n",
        "    return rr\n",
        "\n",
        "def prec_r(ground_truth, prediction):\n",
        "    ground_truth = remove_duplicates(ground_truth)\n",
        "    prediction = remove_duplicates(prediction)\n",
        "    pt = prediction[0:len(ground_truth)]\n",
        "    pr = count_a_in_b_unique(pt, ground_truth) / float(len(pt))\n",
        "    assert 0 <= pr <= 1\n",
        "    return pr\n",
        "\n",
        "\n",
        "def count_a_in_b_unique(a, b):\n",
        "\n",
        "    #returns number of elements of a in b\n",
        "    \n",
        "    count = 0\n",
        "    for el in a:\n",
        "        if el in b:\n",
        "            count += 1\n",
        "    return count\n",
        "    \n",
        "def remove_duplicates(l):\n",
        "    return [list(x) for x in set(tuple(x) for x in l)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulRWPgtRYyRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = {'Precision':precision, \n",
        "           'Recall':recall,\n",
        "           'MRR': mrr,\n",
        "           'r-Precision':prec_r}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCSDdzxJ5VsO",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation for Sequentially revealed user profiles:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tEKlCPVXVc-",
        "colab_type": "code",
        "outputId": "baaf63ba-8f10-42c3-9e00-75ae4bf7cb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "TOPN = 10 \n",
        "STEP = 5\n",
        "r11 = sequential_evaluation(recommender, \n",
        "                            test_sequences=test_sequences, \n",
        "                            given_k=GIVEN_K, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=True, step=STEP)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/11776 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "100%|██████████| 11776/11776 [00:15<00:00, 778.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcUNHIXWX_jP",
        "colab_type": "code",
        "outputId": "ec274f4b-bd53-41ef-e121-1119f6a2b33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "met = list(METRICS.keys())\n",
        "print(\"Sequentially revealed user profiles:\")\n",
        "for i in range(len(r11)):\n",
        "  print(met[i],\" = \",r11[i])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequentially revealed user profiles:\n",
            "Precision  =  0.04570258808005073\n",
            "Recall  =  0.4384538414231781\n",
            "MRR  =  0.2045028013831103\n",
            "r-Precision  =  0.05199296884004565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5x8zzAP7urg",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation for next-item recommendAation with varying recommendation list lengths:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygygYKcYgeE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topn_list = [1, 5, 10, 20, 50, 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V59nZp05wmbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "f8fe8e9b-d9eb-43fd-913e-4d43a1017326"
      },
      "source": [
        "res_list = []\n",
        "\n",
        "for topn in topn_list:\n",
        "    print('Evaluating recommendation lists with length: {}'.format(topn))\n",
        "    res_tmp = sequential_evaluation(recommender,\n",
        "                                               test_sequences=test_sequences,\n",
        "                                               given_k=GIVEN_K,\n",
        "                                               look_ahead=LOOK_AHEAD,\n",
        "                                               evaluation_functions=METRICS.values(),\n",
        "                                               top_n=topn,\n",
        "                                               scroll=True,  # here we average over all profile lengths\n",
        "                                               step=STEP)\n",
        "    mvalues = list(zip(METRICS.keys(), res_tmp))\n",
        "    res_list.append((topn, mvalues))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/11776 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "  0%|          | 33/11776 [00:00<00:38, 303.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:14<00:00, 811.27it/s]\n",
            "  1%|          | 62/11776 [00:00<00:19, 615.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:14<00:00, 805.99it/s]\n",
            "  0%|          | 55/11776 [00:00<00:21, 549.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:15<00:00, 783.46it/s]\n",
            "  0%|          | 51/11776 [00:00<00:23, 508.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:15<00:00, 783.98it/s]\n",
            "  0%|          | 33/11776 [00:00<00:36, 325.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:15<00:00, 766.79it/s]\n",
            "  0%|          | 49/11776 [00:00<00:24, 485.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating recommendation lists with length: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11776/11776 [00:15<00:00, 747.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30fMz0IwxXlR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "6edab7c3-f625-4439-bee4-07366dcf4c79"
      },
      "source": [
        "for i in res_list:\n",
        "  print(\"For list size = \",i[0],\":\")\n",
        "  for j in i[1]:\n",
        "    print(j[0],\" = \",j[1])\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For list size =  1 :\n",
            "Precision  =  0.10016255922785647\n",
            "Recall  =  0.10016255922785647\n",
            "MRR  =  0.10016255922785647\n",
            "r-Precision  =  0.10016255922785647\n",
            "For list size =  5 :\n",
            "Precision  =  0.07160217618062927\n",
            "Recall  =  0.35609378671688835\n",
            "MRR  =  0.19320593116169596\n",
            "r-Precision  =  0.07577799148010507\n",
            "For list size =  10 :\n",
            "Precision  =  0.04570258808005073\n",
            "Recall  =  0.4384538414231781\n",
            "MRR  =  0.2045028013831103\n",
            "r-Precision  =  0.05199296884004565\n",
            "For list size =  20 :\n",
            "Precision  =  0.043975755630879125\n",
            "Recall  =  0.44593918471439803\n",
            "MRR  =  0.20502657312821992\n",
            "r-Precision  =  0.04801187907006319\n",
            "For list size =  50 :\n",
            "Precision  =  0.04216252395606706\n",
            "Recall  =  0.45396725605669763\n",
            "MRR  =  0.20528269003256003\n",
            "r-Precision  =  0.046284018973487086\n",
            "For list size =  100 :\n",
            "Precision  =  0.04133120672150731\n",
            "Recall  =  0.4573902050873502\n",
            "MRR  =  0.20533919209026832\n",
            "r-Precision  =  0.045949924639944725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpWVf8WKx4MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}